---
title: "O algoritmo do assédio: Como o Vale do Silício lucrou com a nossa pele."
date: 2026-01-29 08:00:00 -0300
categories: [IA]
tags: [Grock,IA,Ética,Segurança]
---

Como a IA passou de grande promessa para uma ferramenta que lucra com os padrões misóginos de uma sociedade doente. Quem está sendo afetado com isso? Quais são as consequências?


> "A ideia de que os algoritmos são neutros e de que as plataformas de tecnologia são meros hospedeiros de conteúdo é um dos mitos mais perigosos da nossa era. O design do software é um ato de poder." — Safiya Umoja Noble

Era 1950, as primeiras discussões sobre IA já começavam e o mundo se deslumbrava com mais uma tecnologia descoberta. Com crescimento de estudos e debates sobre a nova ferramenta que mudaria o mundo, a população e os cientistas plantavam um futuro extraordinário, um futuro de máquinas que pensavam. Entre isso, as promessas sobre o futuro começavam: A automação do trabalho braçal e mental, a era do lazer — onde o trabalho era reduzido e a criatividade teria mais espaço —, o cérebro eletrônico como consultor pessoal e por ultimo, a solução de problemas globais — a esperança de uma sociedade que estava assustada.

No entanto, entre as promessas utópicas de 1950 e a realidade do século XXI, a trajetória da IA desviou-se do bem-estar comum para a mercantilização da vulnerabilidade. A tecnologia que deveria "resolver problemas globais" foi treinada com o que há de mais antigo e doente na sociedade: os preconceitos estruturais. Hoje, a esperança de uma sociedade livre do trabalho braçal deu lugar a algoritmos que lucram com a hipersexualização e a vigilância. A trend do "biquíni" na internet é a face visível dessa distorção. Enquanto os pioneiros sonhavam com máquinas que pensavam, o Vale do Silício entregou sistemas que priorizam imagens de corpos femininos expostos para garantir mais tempo de tela e cliques.

O algoritmo não é neutro; ele é um espelho amplificado da sociedade. Quando a máquina aprende que a pele vende mais do que a voz, ela não está apenas automatizando processos, ela está automatizando a desumanização. A promessa de facilidade tornou-se, para as mulheres, a facilitação do abuso digital e a perpetuação de um ciclo de insegurança que alimenta os cofres das grandes empresas de tecnologia.

---

## O caso do X: A democratização do assédio

Recentemente, a plataforma foi inundada por imagens geradas por IA através de prompts degradantes — como "mulheres de biquíni" ou “cobertas com leite” — que expõem mulheres reais de forma não consensual e nojenta.

É importante traçar uma linha clara aqui. Antes da explosão que o Grok — IA do X — havia um mercado de IAs que faziam o mesmo com imagens, mas a diferença era que cobravam pelo acesso a elas. A “brincadeira” suja era feita atrás das cortinas, se você quisesse ver uma mulher nua, você pagava por isso sem precisar do consentimento dela. Agora, no entanto, a IA do X disponibiliza uma amostra gratuita que é apenas o começo de muitos problemas de assédio virtual. Não é de hoje que as DeepFakes são alertadas, geração de imagens falsas tanto de homens quanto de mulheres são geradas diariamente, mas por um motivo especifico a trend do biquíni está afetando em sua maioria corpos de mulheres reais. Aonde termina a fantasia e começa a normalização da mercantilização do corpo feminino em prol do desejo masculino?

[]()
[]()


> "Fotografar pessoas é violá-las, ao vê-las como elas nunca se veem, ao ter delas um conhecimento que elas nunca podem ter; transforma as pessoas em objetos que podem ser possuídos simbolicamente." — Susan Sontag, em Sobre Fotografia.

[]()
[]()

---

> "A tecnologia não é neutra. Nós treinamos máquinas para espelhar nossos próprios preconceitos e nossos desejos mais sombrios, e depois fingimos surpresa quando o resultado é a desumanização." — Inspirado em Ruha Benjamin (Autora de Race After Technology)

Engenheiros dizem “é só um modelo matemático”, mentira! Se o modelo permite a geração de imagens pornográficas de pessoas, principalmente de mulheres e crianças, é porquê não foram colocados travas de segurança que anulassem isso.

**veja só como funciona as travas de segurança:**

**1 - As travas mais básicas são os filtros de input (camadas de entrada)**, antes da IA processar a imagem, o software deveria ter um algoritmo para detecção de objeto e nudez.

**Como funciona:** O modelo detecta os pixels na imagem, ele identifica determinado padrão em imagens de pele humana e formas anatômicas. O esperado era que existisse um Exception que no momento em que o modelo identificasse um rosto ou corpo que possivelmente seria usado para nudez, o sistema bloqueasse o upload na hora.

**2 - filtros de prompt(linguagem):** Além de imagem, o modelo pode identificar termos específicos, quase como tags. O filtro de linguagem é ideal para capturar palavras-chave que contenham expressões sexuais, ou que induzem a algo possivelmente suspeito, como “biquíni”, ou “milk” — que convenhamos… é complicado uma IA identificar duplos sentidos em palavras, mas com todos os filtros ela consegue presumir que é relacionado a outro contexto.

`É fascinante (e levemente desesperador) ver que a gente precisa construir uma babá digital capaz de dizer: “Ei, amigão, talvez usar a palavra “leite” nesse contexto não seja para bater com whey protein, né?”. É a prova de que a tecnologia alcançou a velocidade da luz, mas o bom senso do usuário médio ainda está lá na idade dos ‘dinofauros’. Parabéns, transformaram a maior invenção do século em um fetiche por lacticínios.`

**3 - Travas de contexto e reconhecimento de idade:** A IA deveria ter travas que identificasse se a pessoa é adulta ou há consentimento sobre a imagem. O que seria um pouco complicado por não ter apenas um padrão de adolescentes hoje, então seria mais fácil com crianças, mas não impossível com menores de todas as idades.

Algumas IAs de segurança avançada conseguem identificar um padrão ósseo e corporal específico, então mesmo que a criança não se enquadre em uma aparência infantil, ainda sim há como reconhecer padrões. Além de que algumas poderiam ler contextos com metadados das fotos e ver inconsistências de tempo.

**4 - Filtros de Output (Camada de saída):** Mesmo que a imagem passe pelo input e pelo filtro de imagem, ela deve ser analisada por uma rede neural de segurança para analisar se a imagem há conteúdo pornográfico, e se houver, bloquear imediatamente.

`A IA não é neutra justamente por não haver travas de segurança que tragam segurança para os usuários, há pessoas atrás de códigos que não colocam trava, e a sociedade nunca foi neutra em seus preconceitos e bizarrices. Mas podemos chamar isso de liberdade tecnológica, né?`



---

## A IA vai saber que é uma criança?

Agora é muito importante discutimos sobre filtros de contexto e idade. Até onde eles são eficazes? Tecnicamente a IA não sabe a idade de ninguém, ela observa padrões de visão computacional e aprendizado estatístico. Mas quem se beneficia gerando imagens de crianças?

As falhas de segurança permitem que pessoas predatórias possam fazer da rede sua casa, e do crime seu alimento. Predadores usam técnicas de engenharia social e engenharia de prompt para burlar as IAs de segurança:

**Ofuscação Semântica e Adversarial Prompting:** Eles substituem palavras que seriam bloqueadas pelo filtro por códigos parecidos, como se fosse uma mascara para a IA não entender.

**Jailbreak:** Eles manipulam a IA para gerar conteúdos que pareçam inofensivos, se passando por conteúdos de “arte” ou “educação”.

Se a empresa não investe em Red Teaming(times de ataques que testam a segurança da IA), falhas como essas podem acontecer. E com o avanço da IA a ética dela também precisa ser revisada, ou então teremos uma ferramenta que não apenas gera soluções jamais vistas no mundo, mas também teremos ferramentas de desumanização, destruição e deterioração de qualquer esperança que haja na tecnologia.

---

## A Responsabilidade Legal como algoritmo de punição

Se as empresas dizem que não há controle, por mais que saibamos que há. Então há de se colocar o lembrete: “você é livre para gerar imagem, mas será julgado conforme seu crime.”

O cenário jurídico global e brasileiro está se movendo para transformar o "não sabia" em responsabilidade criminal.

**A tipificação do crime digital:**

No Brasil, a legislação tem progredido no sentido de reconhecer que deepfakes não são apenas montagens, mas sim uma violação da integridade psíquica e moral. Projetos de lei e atualizações no Código Penal (como a Lei 13.718/18) já miram na divulgação de conteúdos sexuais gerados sem consentimento, mas agora o desafio é punir o criador. Se você gerou a imagem criminosa você é o “autor” e a empresa que negligenciou a segurança de travas é a corresponsável.

**Responsabilidade objetiva das plataformas:**

Leis como o AI Act da União Europeia e o PL 2338 no Brasil estabelecem que empresas que lucram com modelos generativos têm o dever de cuidado. Isso significa:

**1 - Watermarking obrigatório:** toda IA que gere imagens deve conter metadados e marca d’agua que identifiquem sua origem.

**2 - Linter de Segurança:** A lei agora exige que o conteúdo seja bloqueado na sua origem, caso não seja, a empresa falhou em proteger dados e garantir a segurança.

**O direito ao esquecimento e a reparação ágil:**

Com a LGPD, o titular dos dados ( a vítima) tem direito de exigir que suas fotos que foram usadas sem permissão sejam apagadas da rede, sob condição de multas pesadíssimas.

`A liberdade de expressão nunca foi um passe livre para a engenharia do abuso. Na tecnologia, gerar um deepfake de uma criança é o erro de design mais imperdoável que um engenheiro pode cometer.`

---

## Onde foi parar a nossa grama do Windows?
[]()
